{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ipynb file serves as the code base for the contiuous optimization project. It was written by Mirjam Brunner and Pascal Kunz.\n",
    "We first start with datacleaning of the a9a and ijcnn datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def process_data_a9a(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            # Split the line by whitespace\n",
    "            elements = line.split()\n",
    "            try:\n",
    "                label = int(elements[0])\n",
    "            except:\n",
    "                continue\n",
    "            features = [int(feature.split(':')[0]) for feature in elements[1:]]\n",
    "            #For some reason, the number of features is not consistent...\n",
    "            if len(features) != 14:\n",
    "                continue\n",
    "            data.append([label] + features)\n",
    "    return data\n",
    "\n",
    "def process_data_ijcnn(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            # Split the line by whitespace\n",
    "            elements = line.split()\n",
    "            try:\n",
    "                label = int(elements[0])\n",
    "            except:\n",
    "                continue\n",
    "            features = [float(feature.split(':')[1]) for feature in elements[2:]]\n",
    "            #For some reason, the number of features is not consistent...\n",
    "            data.append([label] + features)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the newton algorithms in the following section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import data_clean as data_clean\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''\n",
    "    Applies sigmoid function to input\n",
    "    '''\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def log_loss(y_true, y_pred):\n",
    "    '''\n",
    "    Returns Log-loss of y_true and y_pred\n",
    "    '''\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def log_loss_reg(y, y_pred, w, reg_param=0.001):\n",
    "    '''\n",
    "    Returns regularized Log-loss of y_true and y_pred\n",
    "    '''  \n",
    "    loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "    reg_loss = reg_param * np.sum(w**2/(1+w**2))  # Non-convex regularizer    \n",
    "    total_loss = loss + reg_loss    \n",
    "    return total_loss\n",
    "\n",
    "def gradient(y_true, y_pred, X, w, reg_param=0.001, regularized=False):\n",
    "    '''\n",
    "    Computes gradient of log-loss or regularized log-loss\n",
    "    '''  \n",
    "    d_reg_loss = (2 * reg_param * w) * (1 / (1 + w**2)**2)  if regularized else 0\n",
    "    return np.dot(X.T, y_pred - y_true) / len(y_true) + d_reg_loss\n",
    "\n",
    "def hessian(y_pred, X):\n",
    "    '''\n",
    "    returns hessian matrix\n",
    "    '''\n",
    "    diag = np.diag(y_pred * (1 - y_pred))\n",
    "    return np.dot(np.dot(X.T, diag), X) / len(y_pred)\n",
    "\n",
    "def update_weights(method: str, hess, gradient, weights, H=1, step_size=0.01, y_true=None, X = None, loss = None):\n",
    "    '''\n",
    "    Given a specific newton method, we update the weights differently\n",
    "    '''\n",
    "    if method == 'normal_newton':\n",
    "        weights -= np.dot(np.linalg.inv(hess), gradient)\n",
    "        return weights\n",
    "    if method == 'regularized_newton':\n",
    "        # Method from https://arxiv.org/abs/2112.02089\n",
    "        weights -= np.dot(np.linalg.inv(hess + np.sqrt(H*np.linalg.norm(gradient)) * np.eye(len(weights))), gradient)\n",
    "        return weights\n",
    "    if method == 'linesearch_newton':\n",
    "        # Line search strategy\n",
    "        direction = np.dot(np.linalg.inv(hess), gradient)\n",
    "        for i in range(10):\n",
    "            new_weights = weights - step_size * direction\n",
    "            y_pred = sigmoid(np.dot(X, new_weights))\n",
    "            new_loss = log_loss(y_true, y_pred)\n",
    "            if new_loss < loss:\n",
    "                weights = new_weights\n",
    "                break\n",
    "            step_size /= 2\n",
    "        return weights\n",
    "    elif method == 'trust_region_newton':\n",
    "        # Solve the trust region subproblem and update weights\n",
    "        p = solve_trust_region_subproblem(hess, gradient, step_size)\n",
    "        weights += p\n",
    "        return weights\n",
    "    elif method == 'damped_newton':\n",
    "        #Solve damped_newton method according to https://arxiv.org/pdf/2211.00140.pdf\n",
    "        #Lipschitz constant of logistic loss = 1 therefore a_k = (-1 + sqrt(1+2))/1\n",
    "        weights -= np.dot((-1 + np.sqrt(1+2))/1 *np.linalg.inv(hess), gradient)\n",
    "        return weights       \n",
    "    else:\n",
    "        raise ValueError(\"Invalid method specified.\")\n",
    "\n",
    "def solve_trust_region_subproblem(hess, grad, delta):\n",
    "    \"\"\"\n",
    "    Solve the trust region subproblem to obtain the step p.\n",
    "    \"\"\"\n",
    "    # Compute the Newton step\n",
    "    p = np.linalg.solve(hess + delta * np.eye(len(grad)), -grad)\n",
    "\n",
    "    # Apply the trust region constraint\n",
    "    norm_p = np.linalg.norm(p)\n",
    "    if norm_p <= delta:\n",
    "        return p\n",
    "    else:\n",
    "        return (delta / norm_p) * p\n",
    "\n",
    "def newton_method(X, y_true, X_test, y_true_test, num_iterations=100, regularized=False, method='normal_newton', H=1, step_size=0.01):\n",
    "    # Initialize weights\n",
    "    num_features = X.shape[1]\n",
    "    weights = np.zeros(num_features)\n",
    "    accuracy_list = []\n",
    "    logloss_list = []\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "\n",
    "        # Calculate predicted probabilities\n",
    "        y_pred = sigmoid(np.dot(X, weights))\n",
    "        accuracy, logloss = calculate_accuracy(X_test, weights, y_true_test, regularized=regularized)\n",
    "        accuracy_list.append(accuracy)\n",
    "        logloss_list.append(logloss)\n",
    "\n",
    "        # Calculate gradient and Hessian\n",
    "        grad = gradient(y_true, y_pred, X, w=weights, regularized=regularized)\n",
    "        hess = hessian(y_pred, X)\n",
    "        # Update weights using Newton's method\n",
    "        weights = update_weights(method, hess, grad, weights, H=H, y_true=y_true, X=X, loss=logloss, step_size=step_size)    \n",
    "    return weights, accuracy_list, logloss_list\n",
    "\n",
    "def calculate_accuracy(X, weights, labels_test, regularized=False):\n",
    "    # Calculate predicted probabilities for the test dataset using the learned weights\n",
    "    test_pred_probs = sigmoid(np.dot(X, weights))\n",
    "\n",
    "    # Apply a threshold to obtain binary predictions (e.g., threshold = 0.5)\n",
    "    threshold = 0.5\n",
    "    test_predictions = (test_pred_probs >= threshold).astype(int)\n",
    "\n",
    "    # Evaluate the predictions using appropriate evaluation metrics (e.g., accuracy, log loss)\n",
    "    accuracy = np.mean(test_predictions == labels_test)\n",
    "    logloss = log_loss_reg(labels_test, test_pred_probs, weights) if regularized else log_loss(labels_test, test_pred_probs)\n",
    "    \n",
    "    return accuracy, logloss\n",
    "\n",
    "\n",
    "\n",
    "def run(num_iterations, filepath_train, filepath_test, type, regularized=False, method=\"normal_newton\", H=1, step_size=0.01):\n",
    "    if type == \"a9a\":\n",
    "        full_dataset = np.array(data_clean.process_data_a9a(file_path=filepath_train))\n",
    "        full_dataset_test = np.array(data_clean.process_data_a9a(file_path=filepath_test))\n",
    "    else:\n",
    "        full_dataset = np.array(data_clean.process_data_ijcnn(file_path=filepath_train))\n",
    "        full_dataset_test = np.array(data_clean.process_data_ijcnn(file_path=filepath_test))\n",
    "    \n",
    "    labels = full_dataset[:, 0]\n",
    "    labels = np.where(labels == -1, 0, labels)\n",
    "    X = full_dataset[:, 1:]\n",
    "\n",
    "    # Add bias term to the feature matrix of the test dataset\n",
    "    X= np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    labels_test = full_dataset_test[:, 0]\n",
    "    labels_test = np.where(labels_test == -1, 0, labels_test)\n",
    "    X_test = full_dataset_test[:, 1:]\n",
    "\n",
    "    # Add bias term to the feature matrix of the test dataset\n",
    "    X_test= np.c_[np.ones(X_test.shape[0]), X_test]\n",
    "\n",
    "\n",
    "    # Optimize using Newton's method\n",
    "    weights, accuracy, log_loss = newton_method(X, labels, X_test, labels_test, num_iterations=num_iterations, regularized=regularized, method=method, H=H, step_size=step_size)\n",
    " \n",
    "    return weights, accuracy, log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a module for data-cleaning and have defined the actual newton method's we can run some experiments. The following module will run experiments for the methods defined, dataset and number of iterations and store it in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import newton\n",
    "from time import time\n",
    "\n",
    "methods = [\"damped_newton\",\"normal_newton\", \"regularized_newton\", \"linesearch_newton\", \"trust_region_newton\"]\n",
    "\n",
    "\n",
    "def run_experiment(outputfilename, dataset, num_iteration, methods = methods, regularized=False):\n",
    "    if dataset == \"a9a\":\n",
    "        datasetpath = \"Datasets/a9a/\"\n",
    "    elif dataset == \"ijcnn\":\n",
    "        datasetpath = \"Datasets/ijcnn/\"\n",
    "    else:\n",
    "        return \"This dataset does not exist!\"\n",
    "    # Create a CSV file for storing the results\n",
    "    filename = f\"Results/{outputfilename}.csv\"\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Method\", \"Iteration\", \"Accuracy\", \"Log Loss\"])\n",
    "\n",
    "        for method in methods:\n",
    "            print(method)\n",
    "\n",
    "            # Run Native Newton's Code for ijcnn dataset\n",
    "            _, acc, log_loss = newton.run(num_iteration, f'{datasetpath}train.txt', f'{datasetpath}test.txt', type=\"a9a\", method=method, step_size=0.2, H=1, regularized=regularized)\n",
    "\n",
    "            # Write the results to the CSV file\n",
    "            for i in range(len(acc)):\n",
    "                writer.writerow([method, i+1, acc[i], log_loss[i]])\n",
    "\n",
    "            print(\"done\")\n",
    "    print(f\"Results saved to Results/{outputfilename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the experiments by calling the function. Note that this is a computationally expensive task and should not be executred sequentially (and not in a python-notebook for that matter) but rather "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "damped_newton\n",
      "done\n",
      "normal_newton\n",
      "done\n",
      "regularized_newton\n",
      "done\n",
      "linesearch_newton\n",
      "done\n",
      "trust_region_newton\n",
      "done\n",
      "Results saved to Results/a9a_unregularized_2_iterations\n"
     ]
    }
   ],
   "source": [
    "run_experiment(\"a9a_unregularized_2_iterations\", \"a9a\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
